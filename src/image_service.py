"""
å›¾åƒè¯†åˆ«æœåŠ¡æ¨¡å—
å¤„ç†æ¾æçº¿è™«ç—…ç›¸å…³çš„å›¾åƒè¯†åˆ«ã€ç‰¹å¾æå–å’Œåˆ†æ
"""
import logging
import os
from typing import Dict, List, Optional, Tuple, Any
import json
import base64
from io import BytesIO
from pathlib import Path
import numpy as np
from PIL import Image
import cv2

logger = logging.getLogger(__name__)


class EntityRecognitionResult:
    """å®ä½“è¯†åˆ«ç»“æœ"""
    def __init__(self, entity_type: str, entity_name: str, confidence: float, features: Dict[str, Any], bbox: Optional[Tuple] = None):
        self.entity_type = entity_type  # insect, leaf, disease_symptom, tree
        self.entity_name = entity_name  # å…·ä½“åç§°ï¼Œå¦‚"æ¾å¢¨å¤©ç‰›"ã€"æ¾é’ˆå‘é»„"
        self.confidence = confidence    # è¯†åˆ«ç½®ä¿¡åº¦ 0-1
        self.features = features        # ç‰¹å¾å­—å…¸
        self.bbox = bbox               # è¾¹ç•Œæ¡† (x, y, w, h)


class ImageAnalysisService:
    """å›¾åƒåˆ†ææœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–å›¾åƒåˆ†ææœåŠ¡"""
        self.confidence_threshold = 0.5  # è¯†åˆ«ç½®ä¿¡åº¦é˜ˆå€¼
        self.similarity_threshold = 0.6  # ç‰¹å¾ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # æ¾æçº¿è™«ç—…ç›¸å…³å®ä½“çš„æ ‡å‡†ç‰¹å¾åº“
        self.entity_features_db = self._load_entity_features()
        
        logger.info("å›¾åƒåˆ†ææœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _load_entity_features(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½å®ä½“ç‰¹å¾æ•°æ®åº“"""
        # è¿™é‡Œå®šä¹‰äº†æ¾æçº¿è™«ç—…ç›¸å…³å®ä½“çš„æ ‡å‡†ç‰¹å¾
        features_db = {
            # æ˜†è™«ç±»
            "æ¾å¢¨å¤©ç‰›": {
                "type": "insect",
                "features": {
                    "body_color": ["é»‘è‰²", "é»‘è¤è‰²"],
                    "body_length": "13-25mm",
                    "antennae": "é•¿è§¦è§’",
                    "elytra": "é˜ç¿…é»‘è¤è‰²",
                    "size": "ä¸­ç­‰åå¤§",
                    "habitat": "æ¾æ ‘æå¹²"
                },
                "keywords": ["å¤©ç‰›", "æ˜†è™«", "é»‘è‰²", "é•¿è§¦è§’", "é˜ç¿…"]
            },
            "æ—¥æœ¬é•¿å°è ¹": {
                "type": "insect", 
                "features": {
                    "body_color": ["é»„è¤è‰²", "æ£•è¤è‰²"],
                    "body_length": "3-4mm",
                    "size": "å°å‹",
                    "habitat": "æ¾æ ‘çš®ä¸‹"
                },
                "keywords": ["å°è ¹", "æ˜†è™«", "é»„è¤è‰²", "å°å‹"]
            },
            
            # ç—…å®³ç—‡çŠ¶ç±»
            "æ¾é’ˆå‘é»„": {
                "type": "disease_symptom",
                "features": {
                    "color": ["é»„è‰²", "é»„ç»¿è‰²", "è¤é»„è‰²"],
                    "part": "æ¾é’ˆ",
                    "stage": "åˆæœŸç—‡çŠ¶",
                    "distribution": "å±€éƒ¨æˆ–æ•´ä½“"
                },
                "keywords": ["æ¾é’ˆ", "å‘é»„", "å˜è‰²", "é»„è‰²"]
            },
            "æ¾é’ˆå˜çº¢": {
                "type": "disease_symptom",
                "features": {
                    "color": ["çº¢è‰²", "çº¢è¤è‰²", "é”ˆçº¢è‰²"],
                    "part": "æ¾é’ˆ",
                    "stage": "ä¸­æœŸç—‡çŠ¶",
                    "distribution": "æ˜æ˜¾"
                },
                "keywords": ["æ¾é’ˆ", "å˜çº¢", "çº¢è‰²", "çº¢è¤è‰²"]
            },
            "æ¾é’ˆè„±è½": {
                "type": "disease_symptom",
                "features": {
                    "condition": "æ¯èè„±è½",
                    "part": "æ¾é’ˆ",
                    "stage": "åæœŸç—‡çŠ¶",
                    "severity": "ä¸¥é‡"
                },
                "keywords": ["æ¾é’ˆ", "è„±è½", "æ¯è", "æ‰è½"]
            },
            "æ ‘å¹²æµè„‚": {
                "type": "disease_symptom",
                "features": {
                    "substance": "æ ‘è„‚",
                    "color": ["é€æ˜", "ç¥ç€è‰²", "é»„è‰²"],
                    "part": "æ ‘å¹²",
                    "texture": "ç²˜ç¨ "
                },
                "keywords": ["æµè„‚", "æ ‘è„‚", "æ ‘å¹²", "ç²˜ç¨ "]
            },
            
            # æ ‘ç§ç±»
            "é©¬å°¾æ¾": {
                "type": "tree",
                "features": {
                    "needle_length": "8-12cm",
                    "needle_count": "2é’ˆä¸€æŸ",
                    "bark": "çº¢è¤è‰²",
                    "tree_shape": "é«˜å¤§ä¹”æœ¨",
                    "susceptibility": "é«˜æ˜“æ„Ÿæ€§"
                },
                "keywords": ["é©¬å°¾æ¾", "æ¾æ ‘", "2é’ˆ", "çº¢è¤è‰²æ ‘çš®"]
            },
            "é»‘æ¾": {
                "type": "tree",
                "features": {
                    "needle_length": "6-12cm",
                    "needle_count": "2é’ˆä¸€æŸ",
                    "bark": "ç°é»‘è‰²",
                    "tree_shape": "ä¸­ç­‰ä¹”æœ¨",
                    "susceptibility": "ä¸­ç­‰æ˜“æ„Ÿæ€§"
                },
                "keywords": ["é»‘æ¾", "æ¾æ ‘", "2é’ˆ", "ç°é»‘è‰²æ ‘çš®"]
            }
        }
        
        return features_db
    
    async def analyze_image(self, image_data: bytes) -> Dict[str, Any]:
        """
        åˆ†æå›¾åƒï¼Œè¯†åˆ«å…¶ä¸­çš„æ¾æçº¿è™«ç—…ç›¸å…³å®ä½“
        
        Args:
            image_data: å›¾åƒäºŒè¿›åˆ¶æ•°æ®
            
        Returns:
            åŒ…å«è¯†åˆ«ç»“æœçš„å­—å…¸
        """
        try:
            # 1. å›¾åƒé¢„å¤„ç†
            image = self._preprocess_image(image_data)
            
            # 2. å®ä½“è¯†åˆ«ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
            entities = await self._recognize_entities(image)
            
            # 3. ç‰¹å¾æå–
            for entity in entities:
                entity.features = self._extract_features(image, entity)
            
            # 4. ä¸çŸ¥è¯†åº“ç‰¹å¾å¯¹æ¯”
            all_entities = []
            matched_entities = []
            
            for entity in entities:
                similarity = self._calculate_feature_similarity(entity)
                matched_kb_entity = self._find_best_match(entity) if similarity >= self.similarity_threshold else None
                
                # æ·»åŠ è°ƒè¯•æ—¥å¿—
                logger.info(f"å®ä½“: {entity.entity_name}, ç½®ä¿¡åº¦: {entity.confidence}, ç›¸ä¼¼åº¦: {similarity}")
                
                entity_data = {
                    "entity": entity,
                    "similarity": similarity,
                    "matched_kb_entity": matched_kb_entity
                }
                
                all_entities.append(entity_data)
                if similarity >= self.similarity_threshold:
                    matched_entities.append(entity_data)
            
            # 5. æ„å»ºåˆ†æç»“æœ
            result = {
                "image_info": {
                    "size": image.shape[:2],
                    "channels": image.shape[2] if len(image.shape) > 2 else 1
                },
                "detected_entities": [
                    {
                        "type": entity["entity"].entity_type,
                        "name": entity["entity"].entity_name,
                        "confidence": entity["entity"].confidence,
                        "similarity": entity["similarity"],
                        "features": entity["entity"].features,
                        "bbox": entity["entity"].bbox,
                        "matched_kb_entity": entity["matched_kb_entity"]
                    }
                    for entity in all_entities  # æ˜¾ç¤ºæ‰€æœ‰æ£€æµ‹åˆ°çš„å®ä½“ï¼Œä¸åªæ˜¯åŒ¹é…çš„
                ],
                "analysis_summary": {
                    "total_entities": len(entities),
                    "matched_entities": len(matched_entities),
                    "avg_confidence": np.mean([e["entity"].confidence for e in all_entities]) if all_entities else 0
                }
            }
            
            logger.info(f"å›¾åƒåˆ†æå®Œæˆ: æ£€æµ‹åˆ° {len(entities)} ä¸ªå®ä½“, åŒ¹é… {len(matched_entities)} ä¸ª")
            return result
            
        except Exception as e:
            logger.error(f"å›¾åƒåˆ†æå¤±è´¥: {e}")
            raise
    
    def _preprocess_image(self, image_data: bytes) -> np.ndarray:
        """å›¾åƒé¢„å¤„ç†"""
        # å°†bytesè½¬æ¢ä¸ºPIL Image
        image = Image.open(BytesIO(image_data))
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # è°ƒæ•´å¤§å°ï¼ˆå¦‚æœå›¾åƒå¤ªå¤§ï¼‰
        height, width = cv_image.shape[:2]
        if max(height, width) > 1024:
            scale = 1024 / max(height, width)
            new_width = int(width * scale)
            new_height = int(height * scale)
            cv_image = cv2.resize(cv_image, (new_width, new_height))
        
        return cv_image
    
    async def _recognize_entities(self, image: np.ndarray) -> List[EntityRecognitionResult]:
        """
        ä½¿ç”¨AIè¿›è¡ŒçœŸæ­£çš„å›¾åƒå®ä½“è¯†åˆ«
        """
        entities = []
        
        # 1. é¦–å…ˆä½¿ç”¨AIåˆ†æå›¾åƒå†…å®¹
        ai_recognized_objects = await self._ai_recognize_image_content(image)
        
        # 2. åœ¨ç»ˆç«¯è¾“å‡ºAIè¯†åˆ«çš„åŸå§‹ç»“æœ
        logger.info("=" * 60)
        logger.info("ğŸ¤– AIå›¾åƒè¯†åˆ«åŸå§‹ç»“æœ:")
        for obj in ai_recognized_objects:
            logger.info(f"  è¯†åˆ«å¯¹è±¡: {obj['name']}")
            logger.info(f"  ç½®ä¿¡åº¦: {obj['confidence']:.3f}")
            logger.info(f"  ç±»åˆ«: {obj['category']}")
            logger.info(f"  æè¿°: {obj['description']}")
            if obj.get('location'):
                logger.info(f"  ä½ç½®: {obj['location']}")
            logger.info("-" * 40)
        
        # 3. åˆ†æå›¾åƒåŸºæœ¬ç‰¹å¾ä½œä¸ºè¡¥å……
        height, width = image.shape[:2]
        total_pixels = height * width
        
        # è®¡ç®—é¢œè‰²åˆ†å¸ƒ
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        pixels = image_rgb.reshape(-1, 3)
        avg_color = np.mean(pixels, axis=0)
        
        # è®¡ç®—å›¾åƒäº®åº¦
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray)
        
        # è®¡ç®—é¢œè‰²æ–¹å·®ï¼ˆç”¨äºåˆ¤æ–­é¢œè‰²å¤æ‚åº¦ï¼‰
        color_variance = np.var(pixels, axis=0)
        color_complexity = np.mean(color_variance)
        
        # 4. å°†AIè¯†åˆ«ç»“æœä¸çŸ¥è¯†åº“å®ä½“è¿›è¡ŒåŒ¹é…
        logger.info("ğŸ” å¼€å§‹å°†AIè¯†åˆ«ç»“æœä¸çŸ¥è¯†åº“å®ä½“è¿›è¡ŒåŒ¹é…:")
        
        for ai_obj in ai_recognized_objects:
            # å°è¯•åŒ¹é…çŸ¥è¯†åº“å®ä½“
            matched_entity = await self._match_with_knowledge_base(ai_obj)
            
            if matched_entity:
                # åˆ›å»ºåŒ¹é…çš„å®ä½“ç»“æœ
                bbox = self._parse_location_to_bbox(ai_obj.get('location', ''), width, height)
                
                entity_result = EntityRecognitionResult(
                    entity_type=matched_entity['type'],
                    entity_name=f"{matched_entity['name']} (AIè¯†åˆ«: {ai_obj['name']})",
                    confidence=round(ai_obj['confidence'] * matched_entity['similarity'], 1),
                    features={
                        "ai_detected": ai_obj['name'],
                        "ai_confidence": ai_obj['confidence'],
                        "ai_description": ai_obj['description'],
                        "matched_kb_entity": matched_entity['name'],
                        "similarity_score": matched_entity['similarity'],
                        "match_reason": matched_entity['reason']
                    },
                    bbox=bbox
                )
                
                entities.append(entity_result)
                
                logger.info(f"âœ… æˆåŠŸåŒ¹é…: AIè¯†åˆ«'{ai_obj['name']}' -> çŸ¥è¯†åº“'{matched_entity['name']}' (ç›¸ä¼¼åº¦: {matched_entity['similarity']:.3f})")
            else:
                # æœªåŒ¹é…åˆ°çŸ¥è¯†åº“å®ä½“ï¼Œåˆ›å»ºæ–°çš„å®ä½“
                bbox = self._parse_location_to_bbox(ai_obj.get('location', ''), width, height)
                
                entity_result = EntityRecognitionResult(
                    entity_type=ai_obj['category'],
                    entity_name=f"æœªçŸ¥å®ä½“: {ai_obj['name']}",
                    confidence=round(ai_obj['confidence'], 1),
                    features={
                        "ai_detected": ai_obj['name'],
                        "ai_confidence": ai_obj['confidence'],
                        "ai_description": ai_obj['description'],
                        "is_unknown": True
                    },
                    bbox=bbox
                )
                
                entities.append(entity_result)
                
                logger.info(f"âš ï¸  æœªåŒ¹é…åˆ°çŸ¥è¯†åº“å®ä½“: '{ai_obj['name']}' (ç±»åˆ«: {ai_obj['category']})")
        
        # å¦‚æœAIæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½•å¯¹è±¡ï¼Œä½¿ç”¨åŸºç¡€å›¾åƒç‰¹å¾åˆ†æä½œä¸ºåå¤‡
        if not ai_recognized_objects:
            logger.info("âš¡ AIæœªè¯†åˆ«åˆ°å¯¹è±¡ï¼Œä½¿ç”¨å›¾åƒç‰¹å¾åˆ†æä½œä¸ºåå¤‡æ–¹æ¡ˆ")
            backup_entities = await self._fallback_feature_analysis(image, avg_color, brightness, color_complexity)
            entities.extend(backup_entities)
        
        logger.info(f"ğŸ¯ AIå›¾åƒè¯†åˆ«å®Œæˆ: æ£€æµ‹åˆ° {len(entities)} ä¸ªå®ä½“")
        return entities
    
    async def _ai_recognize_image_content(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨AIæœåŠ¡è¯†åˆ«å›¾åƒä¸­çš„å…·ä½“å¯¹è±¡
        """
        try:
            # å¯¼å…¥AIæœåŠ¡
            from ai_service import get_kimi_service
            kimi_service = get_kimi_service()
            
            # åˆ†æå›¾åƒåŸºæœ¬ç‰¹å¾ç”¨äºæç¤º
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pixels = image_rgb.reshape(-1, 3)
            avg_color = np.mean(pixels, axis=0)
            brightness = np.mean(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„AIå®¢æˆ·ç«¯
            if not kimi_service.client:
                logger.warning("Kimiå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡AIå›¾åƒè¯†åˆ«")
                return []
            
            # æ„é€ AIåˆ†ææç¤º
            analysis_prompt = f"""åŸºäºå›¾åƒé¢œè‰²ä¿¡æ¯è¯†åˆ«å›¾åƒä¸­çš„æ‰€æœ‰å¯¹è±¡ï¼š
å¹³å‡é¢œè‰²: RGB({avg_color[0]:.0f}, {avg_color[1]:.0f}, {avg_color[2]:.0f})
å¹³å‡äº®åº¦: {brightness:.0f}

è¯·è¯†åˆ«å›¾åƒä¸­å¯èƒ½å­˜åœ¨çš„å¯¹è±¡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
1. æ¾æçº¿è™«ç—…ç›¸å…³ï¼šæ˜†è™«ï¼ˆå¤©ç‰›ã€å°è ¹ç­‰ï¼‰ã€ç—…ç—‡ï¼ˆæ¾é’ˆå‘é»„/å˜çº¢ã€æµè„‚ç­‰ï¼‰ã€æ ‘ç§ï¼ˆé©¬å°¾æ¾ã€é»‘æ¾ç­‰ï¼‰
2. äº¤é€šå·¥å…·ï¼šæ±½è½¦ã€å¡è½¦ã€è´§è½¦ã€æ‹–è½¦ç­‰
3. å»ºç­‘è®¾æ–½ï¼šæˆ¿å±‹ã€ä»“åº“ã€é“è·¯ã€æ¡¥æ¢ç­‰  
4. è‡ªç„¶ç¯å¢ƒï¼šæ ‘æœ¨ã€æ£®æ—ã€è‰åœ°ã€å¤©ç©ºã€æ°´ä½“ç­‰
5. å·¥ä¸šç‰©å“ï¼šåŸæœ¨ã€æœ¨æã€é›†è£…ç®±ã€æœºæ¢°ç­‰
6. å…¶ä»–æ˜æ˜¾å¯¹è±¡

è¯·è¯†åˆ«æœ€å¯èƒ½çš„5ä¸ªå¯¹è±¡ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œæ ¼å¼ï¼š
å¯¹è±¡åç§°|ç½®ä¿¡åº¦æ•°å€¼|ç±»åˆ«|ç®€çŸ­æè¿°|center

ç±»åˆ«é€‰é¡¹ï¼šinsectã€plantã€disease_symptomã€treeã€vehicleã€buildingã€naturalã€industrialã€other
ç½®ä¿¡åº¦èŒƒå›´ï¼š0.0-1.0

ç¤ºä¾‹ï¼š
è¿è¾“å¡è½¦|0.9|vehicle|å¤§å‹è´§è¿è½¦è¾†|center
åŸæœ¨å †|0.8|industrial|å †ç§¯çš„æœ¨æ|center
æ£®æ—èƒŒæ™¯|0.7|natural|ç»¿è‰²æ¤è¢«|center

è¯·ä¸¥æ ¼æŒ‰ç…§ç¤ºä¾‹æ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜æ–‡å­—ï¼š"""
            
            # è°ƒç”¨AIåˆ†æ
            response = kimi_service.client.chat.completions.create(
                model="moonshot-v1-8k",
                messages=[
                    {
                        "role": "system", 
                        "content": "ä½ æ˜¯ä¸€ä¸ªæ¾æçº¿è™«ç—…è¯†åˆ«ä¸“å®¶ï¼ŒåŸºäºå›¾åƒçš„é¢œè‰²å’Œäº®åº¦ä¿¡æ¯è¯†åˆ«ç›¸å…³å¯¹è±¡ã€‚"
                    },
                    {
                        "role": "user",
                        "content": analysis_prompt
                    }
                ],
                temperature=0.5,
                max_tokens=300
            )
            
            ai_response = response.choices[0].message.content.strip()
            
            # è§£æå“åº”
            return self._parse_ai_response(ai_response)
            
        except Exception as e:
            logger.error(f"AIå›¾åƒè¯†åˆ«å¤±è´¥: {e}")
            # è¿”å›ç©ºåˆ—è¡¨ï¼Œåç»­ä¼šä½¿ç”¨å¤‡ç”¨åˆ†æ
            return []
    
    def _parse_ai_response(self, ai_response: str) -> List[Dict[str, Any]]:
        """
        è§£æAIçš„æ ¼å¼åŒ–å“åº”ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        """
        objects = []
        
        try:
            lines = ai_response.strip().split('\n')
            for line in lines:
                line = line.strip()
                if not line or line.startswith('#') or line.startswith('æ ¹æ®'):
                    continue
                
                # å°è¯•è§£æç®¡é“åˆ†éš”æ ¼å¼: åç§°|ç½®ä¿¡åº¦|ç±»åˆ«|æè¿°|ä½ç½®
                if '|' in line:
                    parts = line.split('|')
                    if len(parts) >= 3:
                        try:
                            name = parts[0].strip()
                            confidence_str = parts[1].strip()
                            
                            # æå–æ•°å­—éƒ¨åˆ†
                            import re
                            confidence_match = re.search(r'(\d+\.?\d*)', confidence_str)
                            confidence = float(confidence_match.group(1)) if confidence_match else 0.7
                            
                            # å¦‚æœç½®ä¿¡åº¦å¤§äº1ï¼Œå‡è®¾æ˜¯ç™¾åˆ†æ¯”ï¼Œè½¬æ¢ä¸º0-1èŒƒå›´
                            if confidence > 1:
                                confidence = confidence / 100
                            
                            category = parts[2].strip() if len(parts) > 2 else 'other'
                            description = parts[3].strip() if len(parts) > 3 else f"AIè¯†åˆ«çš„{name}"
                            location = parts[4].strip() if len(parts) > 4 else 'center'
                            
                            objects.append({
                                'name': name,
                                'confidence': confidence,
                                'category': category,
                                'description': description,
                                'location': location
                            })
                        except (ValueError, IndexError) as e:
                            logger.debug(f"è§£æç®¡é“æ ¼å¼å¤±è´¥: {e}, è·³è¿‡è¡Œ: {line}")
                            continue
                
                # å°è¯•è§£æè‡ªç„¶è¯­è¨€æ ¼å¼
                elif any(keyword in line for keyword in ['æ¾é’ˆ', 'å¤©ç‰›', 'å°è ¹', 'é©¬å°¾æ¾', 'é»‘æ¾', 'æµè„‚', 'å‘é»„', 'å˜çº¢', 
                                                        'å¡è½¦', 'è´§è½¦', 'è½¦è¾†', 'æ±½è½¦', 'è¿è¾“', 'åŸæœ¨', 'æœ¨æ', 'æ£®æ—', 
                                                        'æ ‘æœ¨', 'é“è·¯', 'å»ºç­‘', 'ä»“åº“', 'æœºæ¢°']):
                    # æå–å¯¹è±¡åç§°
                    name = None
                    confidence = 0.7  # é»˜è®¤ç½®ä¿¡åº¦
                    
                    # æ¾æçº¿è™«ç—…ç›¸å…³å¯¹è±¡
                    for keyword in ['æ¾é’ˆå‘é»„', 'æ¾é’ˆå˜çº¢', 'æ¾å¢¨å¤©ç‰›', 'æ—¥æœ¬é•¿å°è ¹', 'é©¬å°¾æ¾', 'é»‘æ¾', 'æ ‘å¹²æµè„‚']:
                        if keyword in line:
                            name = keyword
                            break
                    
                    # äº¤é€šå·¥å…·
                    if not name:
                        for keyword in ['è¿è¾“å¡è½¦', 'è´§è¿å¡è½¦', 'é‡å‹å¡è½¦', 'æ‹–è½¦', 'åŠæŒ‚è½¦', 'è´§è½¦', 'å¡è½¦', 'æ±½è½¦', 'è½¦è¾†']:
                            if keyword in line:
                                name = keyword
                                break
                    
                    # å·¥ä¸šç‰©å“
                    if not name:
                        for keyword in ['åŸæœ¨å †', 'æœ¨æå †', 'åŸæœ¨', 'æœ¨æ', 'é›†è£…ç®±', 'è´§ç‰©']:
                            if keyword in line:
                                name = keyword
                                break
                    
                    # è‡ªç„¶ç¯å¢ƒ
                    if not name:
                        for keyword in ['æ£®æ—èƒŒæ™¯', 'æ ‘æ—', 'æ£®æ—', 'ç»¿è‰²æ¤è¢«', 'æ ‘æœ¨', 'æ¤è¢«']:
                            if keyword in line:
                                name = keyword
                                break
                    
                    # å»ºç­‘è®¾æ–½
                    if not name:
                        for keyword in ['é“è·¯', 'å…¬è·¯', 'å»ºç­‘', 'ä»“åº“', 'å‚æˆ¿']:
                            if keyword in line:
                                name = keyword
                                break
                    
                    # é€šç”¨å…³é”®è¯åŒ¹é…
                    if not name:
                        for keyword in ['æ¾é’ˆ', 'å¤©ç‰›', 'å°è ¹', 'é©¬å°¾æ¾', 'é»‘æ¾', 'æµè„‚']:
                            if keyword in line:
                                name = f"ç–‘ä¼¼{keyword}"
                                break
                    
                    if name:
                        # ç¡®å®šç±»åˆ«
                        if any(k in name for k in ['æ¾é’ˆ', 'æµè„‚']):
                            category = 'disease_symptom'
                        elif any(k in name for k in ['å¤©ç‰›', 'å°è ¹']):
                            category = 'insect'
                        elif any(k in name for k in ['æ¾', 'æ ‘']):
                            category = 'tree'
                        elif any(k in name for k in ['å¡è½¦', 'è´§è½¦', 'è½¦è¾†', 'æ±½è½¦', 'è¿è¾“', 'æ‹–è½¦']):
                            category = 'vehicle'
                        elif any(k in name for k in ['åŸæœ¨', 'æœ¨æ', 'é›†è£…ç®±', 'è´§ç‰©']):
                            category = 'industrial'
                        elif any(k in name for k in ['æ£®æ—', 'æ ‘æ—', 'æ¤è¢«']):
                            category = 'natural'
                        elif any(k in name for k in ['é“è·¯', 'å»ºç­‘', 'ä»“åº“', 'å‚æˆ¿']):
                            category = 'building'
                        else:
                            category = 'other'
                        
                        # å°è¯•æå–ç½®ä¿¡åº¦
                        import re
                        confidence_match = re.search(r'(\d+\.?\d*)%?', line)
                        if confidence_match:
                            confidence = float(confidence_match.group(1))
                            if confidence > 1:
                                confidence = confidence / 100
                        
                        objects.append({
                            'name': name,
                            'confidence': confidence,
                            'category': category,
                            'description': line,
                            'location': 'center'
                        })
                        
        except Exception as e:
            logger.warning(f"AIå“åº”è§£æå¤±è´¥: {e}, ä½¿ç”¨æ–‡æœ¬è§£æ")
            return self._parse_ai_text_response(ai_response)
        
        # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•å¯¹è±¡ï¼Œå°è¯•æ–‡æœ¬è§£æ
        if not objects:
            return self._parse_ai_text_response(ai_response)
        
        return objects
    
    def _parse_ai_text_response(self, text: str) -> List[Dict[str, Any]]:
        """
        è§£æAIçš„æ–‡æœ¬å“åº”ï¼Œæå–å¯¹è±¡ä¿¡æ¯
        """
        objects = []
        
        # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
        lines = text.split('\n')
        current_obj = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æŸ¥æ‰¾å¯¹è±¡åç§°
            if 'è¯†åˆ«' in line or 'å‘ç°' in line or 'æ£€æµ‹' in line:
                if current_obj:
                    objects.append(current_obj)
                    current_obj = {}
                
                # æå–å¯¹è±¡åç§°
                for keyword in ['æ¾é’ˆ', 'å¤©ç‰›', 'å°è ¹', 'é©¬å°¾æ¾', 'é»‘æ¾', 'æµè„‚', 'å‘é»„', 'å˜çº¢']:
                    if keyword in line:
                        current_obj['name'] = keyword
                        current_obj['confidence'] = 0.7  # é»˜è®¤ç½®ä¿¡åº¦
                        if keyword in ['æ¾é’ˆ', 'é©¬å°¾æ¾', 'é»‘æ¾']:
                            current_obj['category'] = 'plant'
                        elif keyword in ['å¤©ç‰›', 'å°è ¹']:
                            current_obj['category'] = 'insect'
                        else:
                            current_obj['category'] = 'disease_symptom'
                        current_obj['description'] = line
                        current_obj['location'] = 'center'
                        break
        
        if current_obj:
            objects.append(current_obj)
        
        return objects
    
    async def _match_with_knowledge_base(self, ai_obj: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°†AIè¯†åˆ«çš„å¯¹è±¡ä¸çŸ¥è¯†åº“å®ä½“è¿›è¡ŒåŒ¹é…
        """
        best_match = None
        best_similarity = 0.0
        
        ai_name = ai_obj['name'].lower()
        ai_category = ai_obj.get('category', '')
        
        for kb_name, kb_entity in self.entity_features_db.items():
            similarity = 0.0
            
            # åç§°ç›¸ä¼¼åº¦
            if ai_name in kb_name.lower() or any(keyword in ai_name for keyword in kb_entity.get('keywords', [])):
                similarity += 0.6
            
            # ç±»åˆ«åŒ¹é…
            if ai_category == kb_entity.get('type'):
                similarity += 0.3
            
            # æè¿°åŒ¹é…
            ai_desc = ai_obj.get('description', '').lower()
            if any(keyword in ai_desc for keyword in kb_entity.get('keywords', [])):
                similarity += 0.1
            
            if similarity > best_similarity and similarity > 0.5:  # æœ€ä½åŒ¹é…é˜ˆå€¼
                best_similarity = similarity
                best_match = {
                    'name': kb_name,
                    'type': kb_entity.get('type', 'unknown'),
                    'similarity': similarity,
                    'reason': f"åç§°åŒ¹é…åº¦: {similarity:.2f}"
                }
        
        return best_match
    
    def _parse_location_to_bbox(self, location: str, width: int, height: int) -> Tuple[int, int, int, int]:
        """
        å°†ä½ç½®æè¿°è½¬æ¢ä¸ºè¾¹ç•Œæ¡†
        """
        location = location.lower()
        
        if 'left' in location:
            return (0, height//4, width//2, height//2)
        elif 'right' in location:
            return (width//2, height//4, width//2, height//2)
        elif 'top' in location:
            return (width//4, 0, width//2, height//2)
        elif 'bottom' in location:
            return (width//4, height//2, width//2, height//2)
        else:  # center or unknown
            return (width//4, height//4, width//2, height//2)
    
    async def _fallback_feature_analysis(self, image: np.ndarray, avg_color: np.ndarray, brightness: float, color_complexity: float) -> List[EntityRecognitionResult]:
        """
        å¤‡ç”¨çš„å›¾åƒç‰¹å¾åˆ†ææ–¹æ³•
        """
        entities = []
        height, width = image.shape[:2]
        total_pixels = height * width
        
        logger.info("ä½¿ç”¨å¤‡ç”¨å›¾åƒç‰¹å¾åˆ†æ...")
        
        # åŸºäºå›¾åƒç‰¹å¾ç”Ÿæˆå®ä½“
        detected_entities = []
        
        # 1. æ ¹æ®é¢œè‰²ç‰¹å¾åˆ¤æ–­å¯èƒ½çš„å¯¹è±¡
        
        # è½¦è¾†æ£€æµ‹ï¼ˆåŸºäºé‡‘å±è‰²è°ƒå’Œå‡ ä½•ç‰¹å¾ï¼‰
        if avg_color[0] > 80 and avg_color[1] > 80 and avg_color[2] > 80:  # è¾ƒäº®çš„é¢œè‰²ï¼Œå¯èƒ½æ˜¯è½¦è¾†
            if brightness > 120 and color_complexity > 800:  # äº®åº¦é«˜ä¸”å¤æ‚åº¦é€‚ä¸­
                detected_entities.append({
                    "type": "vehicle",
                    "name": f"ç–‘ä¼¼è¿è¾“è½¦è¾† (äº®åº¦:{brightness:.1f})",
                    "confidence": min(0.9, 0.6 + brightness / 300),
                    "bbox": (width//4, height//3, width//2, height//3),
                    "raw_features": {
                        "dominant_color": "é‡‘å±è‰²è°ƒ",
                        "size_estimate": "å¤§å‹",
                        "detection_basis": f"é«˜äº®åº¦ç‰¹å¾,äº®åº¦:{brightness:.0f}"
                    }
                })
        
        # å·¥ä¸šåŸæœ¨æ£€æµ‹ï¼ˆåŸºäºæ£•è‰²è°ƒï¼‰
        brown_score = avg_color[0] * 0.6 + avg_color[1] * 0.8 - avg_color[2] * 0.4
        if brown_score > 50 and avg_color[0] > 100:
            detected_entities.append({
                "type": "industrial",
                "name": f"ç–‘ä¼¼åŸæœ¨å † (æ£•è‰²è¯„åˆ†:{brown_score:.1f})",
                "confidence": min(0.85, 0.5 + brown_score / 200),
                "bbox": (0, height//3, width, height//3),
                "raw_features": {
                    "dominant_color": "æ£•æœ¨è‰²",
                    "texture": "ç²—ç³™",
                    "detection_basis": f"æœ¨æè‰²å½©ç‰¹å¾,è¯„åˆ†:{brown_score:.1f}"
                }
            })
        
        # æ˜†è™«æ£€æµ‹ï¼ˆåŸºäºæš—è‰²ç‰¹å¾ï¼‰
        if avg_color[0] < 100 and avg_color[1] < 100:  # æš—è‰²è°ƒ
            if color_complexity > 1000:  # é¢œè‰²å˜åŒ–è¾ƒå¤š
                detected_entities.append({
                    "type": "insect",
                    "name": f"ç–‘ä¼¼æ¾å¢¨å¤©ç‰› (ç½®ä¿¡åº¦åŸºäºæš—è‰²ç‰¹å¾)",
                    "confidence": min(0.95, 0.6 + color_complexity / 5000),
                    "bbox": self._find_dark_regions(image),
                    "raw_features": {
                        "dominant_color": "é»‘è¤è‰²",
                        "size_estimate": "ä¸­ç­‰",
                        "detection_basis": f"æš—è‰²åŒºåŸŸæ£€æµ‹,å¤æ‚åº¦:{color_complexity:.0f}"
                    }
                })
        
        # è‡ªç„¶ç¯å¢ƒæ£€æµ‹ï¼ˆåŸºäºç»¿è‰²ç‰¹å¾ï¼‰
        if avg_color[1] > avg_color[0] and avg_color[1] > avg_color[2]:  # ç»¿è‰²å ä¸»å¯¼
            detected_entities.append({
                "type": "natural",
                "name": f"ç–‘ä¼¼æ£®æ—èƒŒæ™¯ (ç»¿è‰²ç‰¹å¾)",
                "confidence": 0.8,
                "bbox": (0, 0, width, height//2),
                "raw_features": {
                    "dominant_color": "ç»¿è‰²",
                    "environment_type": "æ£®æ—",
                    "detection_basis": "ç»¿è‰²æ¤è¢«ç‰¹å¾"
                }
            })
        
        # 2. æ ¹æ®çº¢è‰²/æ©™è‰²åˆ¤æ–­ç—…å®³ç—‡çŠ¶
        red_ratio = avg_color[0] / (avg_color[1] + avg_color[2] + 1)
        if red_ratio > 1.2 or avg_color[0] > 150:  # çº¢è‰²æˆ–æ©™è‰²å ä¸»å¯¼
            detected_entities.append({
                "type": "disease_symptom",
                "name": f"ç–‘ä¼¼æ¾æçº¿è™«ç—…ç—‡çŠ¶ (çº¢è‰²æ¯”ç‡:{red_ratio:.2f})",
                "confidence": min(0.9, 0.5 + red_ratio * 0.3),
                "bbox": self._find_colored_regions(image, "red"),
                "raw_features": {
                    "dominant_color": "çº¢æ©™è‰²",
                    "intensity": f"é«˜ (æ¯”ç‡:{red_ratio:.2f})",
                    "distribution": "å±€éƒ¨" if color_complexity > 2000 else "å¹¿æ³›"
                }
            })
        
        # 3. æ ¹æ®é»„è‰²ç‰¹å¾åˆ¤æ–­æ¾é’ˆå‘é»„
        yellow_score = (avg_color[0] + avg_color[1]) / 2 - avg_color[2]
        if yellow_score > 20:
            detected_entities.append({
                "type": "disease_symptom",
                "name": f"ç–‘ä¼¼æ¾é’ˆå‘é»„ (é»„è‰²è¯„åˆ†:{yellow_score:.1f})",
                "confidence": min(0.85, 0.4 + yellow_score / 100),
                "bbox": self._find_colored_regions(image, "yellow"), 
                "raw_features": {
                    "dominant_color": "é»„è‰²",
                    "severity": "ä¸­ç­‰" if yellow_score < 50 else "ä¸¥é‡",
                    "pattern": "é’ˆçŠ¶"
                }
            })
        
        # 4. æ ¹æ®æ£•è‰²/ç»¿è‰²åˆ¤æ–­æ ‘æœ¨
        if avg_color[1] > avg_color[0] * 0.7:  # æœ‰ä¸€å®šç»¿è‰²æˆåˆ†
            tree_confidence = 0.6 + (avg_color[1] - avg_color[0]) / 255 * 0.3
            tree_type = self._classify_tree_type(avg_color, color_complexity)
            detected_entities.append({
                "type": "tree",
                "name": f"ç–‘ä¼¼{tree_type} (ç»¿è‰²ç‰¹å¾)",
                "confidence": min(0.9, tree_confidence),
                "bbox": (0, 0, width, height),
                "raw_features": {
                    "bark_pattern": "çºµå‘" if color_complexity > 1500 else "å…‰æ»‘",
                    "size_category": "å¤§å‹" if total_pixels > 500000 else "ä¸­å‹",
                    "health_status": "å¥åº·" if avg_color[1] > 120 else "å¯ç–‘"
                }
            })
        
        # 5. å¦‚æœæ˜¯æš—è‰²è°ƒå›¾åƒï¼Œå¯èƒ½æœ‰ç¯å¢ƒå› å­
        if brightness < 100:
            detected_entities.append({
                "type": "environment",
                "name": f"é˜´æš—ç¯å¢ƒå› å­ (äº®åº¦:{brightness:.1f})",
                "confidence": 0.7,
                "bbox": None,
                "raw_features": {
                    "light_condition": "ä½å…‰ç…§",
                    "humidity_indicator": "å¯èƒ½åé«˜",
                    "risk_factor": "ç—…å®³ä¼ æ’­é£é™©å¢åŠ "
                }
            })
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ç‰¹å¾ï¼Œè¿”å›é»˜è®¤å®ä½“
        if not detected_entities:
            detected_entities.append({
                "type": "tree", 
                "name": f"æœªåˆ†ç±»æ¤ç‰© (å›¾åƒç‰¹å¾ä¸æ˜æ˜¾)",
                "confidence": 0.5,
                "bbox": (0, 0, width, height),
                "raw_features": {
                    "avg_brightness": f"{brightness:.1f}",
                    "color_complexity": f"{color_complexity:.1f}",
                    "analysis_note": "éœ€è¦æ›´æ¸…æ™°çš„å›¾åƒ"
                }
            })
        
        # è½¬æ¢ä¸ºEntityRecognitionResultå¯¹è±¡
        for detection in detected_entities:
            entity = EntityRecognitionResult(
                entity_type=detection["type"],
                entity_name=detection["name"],
                confidence=detection["confidence"],
                features=detection["raw_features"],
                bbox=detection["bbox"]
            )
            entities.append(entity)
        
        logger.info(f"åŸºäºå›¾åƒç‰¹å¾æ£€æµ‹åˆ° {len(entities)} ä¸ªå®ä½“")
        return entities
    
    def _find_dark_regions(self, image: np.ndarray) -> Tuple[int, int, int, int]:
        """æ‰¾åˆ°å›¾åƒä¸­çš„æš—è‰²åŒºåŸŸ"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # æ‰¾åˆ°æœ€æš—çš„åŒºåŸŸ
        _, dark_mask = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY_INV)
        contours, _ = cv2.findContours(dark_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            # æ‰¾åˆ°æœ€å¤§çš„æš—è‰²åŒºåŸŸ
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            return (x, y, w, h)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›å›¾åƒä¸­å¤®çš„ä¸€ä¸ªåŒºåŸŸ
        h, w = image.shape[:2]
        return (w//4, h//4, w//2, h//2)
    
    def _find_colored_regions(self, image: np.ndarray, color_type: str) -> Tuple[int, int, int, int]:
        """æ‰¾åˆ°ç‰¹å®šé¢œè‰²çš„åŒºåŸŸ"""
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        if color_type == "red":
            # çº¢è‰²çš„HSVèŒƒå›´
            lower1 = np.array([0, 50, 50])
            upper1 = np.array([10, 255, 255])
            lower2 = np.array([170, 50, 50])
            upper2 = np.array([180, 255, 255])
            mask1 = cv2.inRange(hsv, lower1, upper1)
            mask2 = cv2.inRange(hsv, lower2, upper2)
            mask = cv2.bitwise_or(mask1, mask2)
        elif color_type == "yellow":
            # é»„è‰²çš„HSVèŒƒå›´
            lower = np.array([20, 50, 50])
            upper = np.array([30, 255, 255])
            mask = cv2.inRange(hsv, lower, upper)
        else:
            # é»˜è®¤è¿”å›å›¾åƒä¸­å¤®åŒºåŸŸ
            h, w = image.shape[:2]
            return (w//4, h//4, w//2, h//2)
        
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            # æ‰¾åˆ°æœ€å¤§çš„é¢œè‰²åŒºåŸŸ
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            return (x, y, w, h)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›å›¾åƒä¸­å¤®çš„ä¸€ä¸ªåŒºåŸŸ
        h, w = image.shape[:2]
        return (w//3, h//3, w//3, h//3)
    
    def _classify_tree_type(self, avg_color: np.ndarray, color_complexity: float) -> str:
        """æ ¹æ®é¢œè‰²ç‰¹å¾åˆ†ç±»æ ‘ç§"""
        red, green, blue = avg_color
        
        # é©¬å°¾æ¾ç‰¹å¾ï¼šåçº¢è¤è‰²
        if red > green and red > 100:
            return "é©¬å°¾æ¾"
        
        # é»‘æ¾ç‰¹å¾ï¼šåæš—è‰²
        elif red < 80 and green < 80 and blue < 80:
            return "é»‘æ¾"
        
        # æ¹¿åœ°æ¾ç‰¹å¾ï¼šé¢œè‰²è¾ƒäº®
        elif green > 120 and color_complexity < 1000:
            return "æ¹¿åœ°æ¾"
        
        # è½å¶æ¾ç‰¹å¾ï¼šå¦‚æœæœ‰é»„è‰²å€¾å‘
        elif red > 100 and green > 100 and blue < 80:
            return "è½å¶æ¾"
        
        # é»˜è®¤
        else:
            return "æœªçŸ¥æ¾æ ‘"
    
    def _extract_features(self, image: np.ndarray, entity: EntityRecognitionResult) -> Dict[str, Any]:
        """ç‰¹å¾æå–"""
        features = entity.features.copy()
        
        # å¦‚æœæœ‰è¾¹ç•Œæ¡†ï¼Œæå–è¯¥åŒºåŸŸçš„ç‰¹å¾
        if entity.bbox:
            x, y, w, h = entity.bbox
            roi = image[y:y+h, x:x+w]
            
            # æå–é¢œè‰²ç‰¹å¾
            features.update(self._extract_color_features(roi))
            
            # æå–å½¢çŠ¶ç‰¹å¾
            features.update(self._extract_shape_features(roi))
            
            # æå–çº¹ç†ç‰¹å¾
            features.update(self._extract_texture_features(roi))
        
        return features
    
    def _extract_color_features(self, roi: np.ndarray) -> Dict[str, Any]:
        """æå–é¢œè‰²ç‰¹å¾"""
        # è®¡ç®—ä¸»è¦é¢œè‰²
        roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
        pixels = roi_rgb.reshape(-1, 3)
        
        # è®¡ç®—å¹³å‡é¢œè‰²
        avg_color = np.mean(pixels, axis=0)
        
        # å°†RGBå€¼è½¬æ¢ä¸ºé¢œè‰²æè¿°
        color_name = self._rgb_to_color_name(avg_color)
        
        return {
            "avg_rgb": avg_color.tolist(),
            "dominant_color": color_name,
            "color_variance": np.var(pixels, axis=0).tolist()
        }
    
    def _extract_shape_features(self, roi: np.ndarray) -> Dict[str, Any]:
        """æå–å½¢çŠ¶ç‰¹å¾"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        
        # äºŒå€¼åŒ–
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            # æ‰¾åˆ°æœ€å¤§è½®å»“
            max_contour = max(contours, key=cv2.contourArea)
            
            # è®¡ç®—å½¢çŠ¶ç‰¹å¾
            area = cv2.contourArea(max_contour)
            perimeter = cv2.arcLength(max_contour, True)
            
            # é•¿å®½æ¯”
            x, y, w, h = cv2.boundingRect(max_contour)
            aspect_ratio = w / h if h > 0 else 1
            
            return {
                "area": area,
                "perimeter": perimeter,
                "aspect_ratio": aspect_ratio,
                "compactness": (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0
            }
        
        return {"area": 0, "perimeter": 0, "aspect_ratio": 1, "compactness": 0}
    
    def _extract_texture_features(self, roi: np.ndarray) -> Dict[str, Any]:
        """æå–çº¹ç†ç‰¹å¾"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        
        # è®¡ç®—æ¢¯åº¦
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        # æ¢¯åº¦å¹…å€¼
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        return {
            "texture_roughness": np.std(gradient_magnitude),
            "texture_uniformity": 1.0 / (1.0 + np.var(gray)),
            "brightness": np.mean(gray)
        }
    
    def _rgb_to_color_name(self, rgb: np.ndarray) -> str:
        """å°†RGBå€¼è½¬æ¢ä¸ºé¢œè‰²åç§°"""
        r, g, b = rgb
        
        # ç®€å•çš„é¢œè‰²åˆ†ç±»
        if r > 200 and g > 200 and b < 100:
            return "é»„è‰²"
        elif r > 200 and g < 100 and b < 100:
            return "çº¢è‰²"
        elif r < 100 and g > 150 and b < 100:
            return "ç»¿è‰²"
        elif r < 100 and g < 100 and b > 150:
            return "è“è‰²"
        elif r < 100 and g < 100 and b < 100:
            return "é»‘è‰²"
        elif r > 200 and g > 200 and b > 200:
            return "ç™½è‰²"
        elif r > 100 and g < 80 and b < 80:
            return "çº¢è¤è‰²"
        elif r > 150 and g > 100 and b < 80:
            return "é»„è¤è‰²"
        elif r < 80 and g < 80 and b < 80:
            return "é»‘è¤è‰²"
        else:
            return "è¤è‰²"
    
    def _calculate_feature_similarity(self, entity: EntityRecognitionResult) -> float:
        """è®¡ç®—å®ä½“ç‰¹å¾ä¸çŸ¥è¯†åº“çš„ç›¸ä¼¼åº¦"""
        best_similarity = 0.0
        
        # æ ¹æ®å®ä½“ç±»å‹ï¼Œä¸å¯¹åº”çš„çŸ¥è¯†åº“å®ä½“æ¯”è¾ƒ
        for kb_name, kb_entity in self.entity_features_db.items():
            if kb_entity["type"] == entity.entity_type:
                similarity = self._compare_features(entity.features, kb_entity["features"])
                best_similarity = max(best_similarity, similarity)
        
        return best_similarity
    
    def _compare_features(self, features1: Dict[str, Any], features2: Dict[str, Any]) -> float:
        """æ¯”è¾ƒä¸¤ä¸ªç‰¹å¾å­—å…¸çš„ç›¸ä¼¼åº¦"""
        similarity_scores = []
        
        # æ¯”è¾ƒé¢œè‰²ç‰¹å¾
        if "dominant_color" in features1 and "body_color" in features2:
            color1 = features1["dominant_color"]
            colors2 = features2["body_color"] if isinstance(features2["body_color"], list) else [features2["body_color"]]
            color_match = 1.0 if color1 in colors2 else 0.5 if any(c in color1 for c in colors2) else 0.0
            similarity_scores.append(color_match)
        
        # æ¯”è¾ƒå¤§å°ç‰¹å¾
        if "area" in features1 and "body_length" in features2:
            # è¿™é‡Œå¯ä»¥åŸºäºé¢ç§¯æ¨æ–­å¤§å°ç±»åˆ«
            area = features1["area"]
            if area > 10000:
                size_category = "å¤§å‹"
            elif area > 5000:
                size_category = "ä¸­ç­‰"
            else:
                size_category = "å°å‹"
            
            # ç®€å•åŒ¹é…
            if "å°å‹" in features2["body_length"] and size_category == "å°å‹":
                similarity_scores.append(0.8)
            elif "ä¸­ç­‰" in str(features2) and size_category == "ä¸­ç­‰":
                similarity_scores.append(0.8)
            else:
                similarity_scores.append(0.3)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯æ¯”è¾ƒçš„ç‰¹å¾ï¼Œè¿”å›åŸºç¡€ç›¸ä¼¼åº¦
        if not similarity_scores:
            return 0.5
        
        return np.mean(similarity_scores)
    
    def _find_best_match(self, entity: EntityRecognitionResult) -> Optional[str]:
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„çŸ¥è¯†åº“å®ä½“"""
        best_match = None
        best_similarity = 0.0
        
        for kb_name, kb_entity in self.entity_features_db.items():
            if kb_entity["type"] == entity.entity_type:
                similarity = self._compare_features(entity.features, kb_entity["features"])
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = kb_name
        
        return best_match if best_similarity > self.similarity_threshold else None


class KnowledgeInferenceService:
    """çŸ¥è¯†æ¨ç†æœåŠ¡"""
    
    def __init__(self, db_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–çŸ¥è¯†æ¨ç†æœåŠ¡
        
        Args:
            db_config: æ•°æ®åº“é…ç½®
        """
        self.db_config = db_config
        self.confidence_threshold = 0.5
        
    async def analyze_disease_prediction(self, detected_entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åŸºäºæ£€æµ‹åˆ°çš„å®ä½“è¿›è¡Œç—…å®³é¢„æµ‹åˆ†æ
        
        Args:
            detected_entities: æ£€æµ‹åˆ°çš„å®ä½“åˆ—è¡¨
            
        Returns:
            é¢„æµ‹åˆ†æç»“æœ
        """
        from ai_service import get_kimi_service
        import pymysql
        
        try:
            # 1. å®ä½“åˆ†ç±»
            insects = [e for e in detected_entities if e["type"] == "insect"]
            symptoms = [e for e in detected_entities if e["type"] == "disease_symptom"]
            trees = [e for e in detected_entities if e["type"] == "tree"]
            
            # 2. ä»çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢ç›¸å…³ä¿¡æ¯
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                
                # æŸ¥è¯¢ç›¸å…³çš„ç–¾ç—…ä¿¡æ¯
                disease_info = await self._query_disease_info(cursor, detected_entities)
                
                # æŸ¥è¯¢ä¼ æ’­è·¯å¾„
                transmission_info = await self._query_transmission_paths(cursor, insects)
                
                # æŸ¥è¯¢é˜²æ²»æªæ–½
                treatment_info = await self._query_treatment_methods(cursor, disease_info)
            
            # 3. ä½¿ç”¨AIè¿›è¡Œæ·±åº¦åˆ†æ
            kimi = get_kimi_service()
            ai_analysis = await self._get_ai_analysis(kimi, detected_entities, disease_info)
            
            # 4. ç”Ÿæˆé¢„æµ‹ç»“æœ
            prediction = {
                "detected_summary": {
                    "insects_count": len(insects),
                    "symptoms_count": len(symptoms), 
                    "trees_count": len(trees),
                    "entities": detected_entities
                },
                "disease_prediction": {
                    "likely_diseases": disease_info.get("diseases", []),
                    "confidence": self._calculate_prediction_confidence(detected_entities, disease_info),
                    "risk_level": self._assess_risk_level(detected_entities, disease_info)
                },
                "transmission_analysis": transmission_info,
                "recommended_actions": treatment_info,
                "ai_insights": ai_analysis,
                "knowledge_gaps": await self._identify_knowledge_gaps(detected_entities)
            }
            
            return prediction
            
        except Exception as e:
            logger.error(f"ç—…å®³é¢„æµ‹åˆ†æå¤±è´¥: {e}")
            raise
    
    def _get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        import pymysql
        return pymysql.connect(**self.db_config)
    
    async def _query_disease_info(self, cursor, entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æŸ¥è¯¢ç–¾ç—…ç›¸å…³ä¿¡æ¯"""
        diseases = []
        
        # æ ¹æ®ç—‡çŠ¶æŸ¥è¯¢å¯èƒ½çš„ç–¾ç—…
        for entity in entities:
            if entity["type"] == "disease_symptom":
                entity_name = entity["matched_kb_entity"] or entity["name"]
                
                # æŸ¥è¯¢ä¸ç—‡çŠ¶ç›¸å…³çš„ç–¾ç—…
                cursor.execute("""
                    SELECT DISTINCT tail_entity as disease
                    FROM knowledge_triples 
                    WHERE head_entity = %s AND relation IN ('ç—‡çŠ¶', 'è¡¨ç°', 'å¯¼è‡´')
                """, (entity_name,))
                
                for row in cursor.fetchall():
                    if row["disease"] not in diseases:
                        diseases.append(row["disease"])
        
        return {"diseases": diseases}
    
    async def _query_transmission_paths(self, cursor, insects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æŸ¥è¯¢ä¼ æ’­è·¯å¾„ä¿¡æ¯"""
        transmission_paths = []
        
        for insect in insects:
            insect_name = insect["matched_kb_entity"] or insect["name"]
            
            # æŸ¥è¯¢æ˜†è™«çš„ä¼ æ’­ä½œç”¨
            cursor.execute("""
                SELECT head_entity, relation, tail_entity
                FROM knowledge_triples 
                WHERE head_entity = %s AND relation IN ('ä¼ æ’­', 'æºå¸¦', 'åª’ä»‹')
            """, (insect_name,))
            
            for row in cursor.fetchall():
                transmission_paths.append({
                    "vector": row["head_entity"],
                    "relation": row["relation"], 
                    "pathogen": row["tail_entity"]
                })
        
        return {"paths": transmission_paths}
    
    async def _query_treatment_methods(self, cursor, disease_info: Dict[str, Any]) -> Dict[str, Any]:
        """æŸ¥è¯¢é˜²æ²»æ–¹æ³•"""
        treatments = []
        
        for disease in disease_info.get("diseases", []):
            # æŸ¥è¯¢é˜²æ²»æ–¹æ³•
            cursor.execute("""
                SELECT tail_entity as treatment
                FROM knowledge_triples 
                WHERE head_entity = %s AND relation IN ('é˜²æ²»', 'æ²»ç–—', 'æ§åˆ¶')
            """, (disease,))
            
            for row in cursor.fetchall():
                treatments.append({
                    "disease": disease,
                    "treatment": row["treatment"]
                })
        
        return {"treatments": treatments}
    
    async def _get_ai_analysis(self, kimi_service, entities: List[Dict[str, Any]], disease_info: Dict[str, Any]) -> str:
        """è·å–AIæ·±åº¦åˆ†æ"""
        try:
            # æ„å»ºåˆ†æprompt
            entities_desc = ", ".join([f"{e['name']}({e['confidence']:.2f})" for e in entities])
            diseases_desc = ", ".join(disease_info.get("diseases", ["æœªçŸ¥"]))
            
            prompt = f"""ä½œä¸ºæ¾æçº¿è™«ç—…ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æƒ…å†µï¼š

æ£€æµ‹åˆ°çš„å®ä½“ï¼š{entities_desc}
å¯èƒ½çš„ç–¾ç—…ï¼š{diseases_desc}

è¯·æä¾›ï¼š
1. ç»¼åˆè¯Šæ–­æ„è§
2. é£é™©è¯„ä¼°
3. ç´§æ€¥ç¨‹åº¦è¯„çº§
4. å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨

è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”ï¼Œä¸è¶…è¿‡200å­—ã€‚"""

            if hasattr(kimi_service, 'client') and kimi_service.client:
                response = kimi_service.client.chat.completions.create(
                    model="moonshot-v1-8k",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯æ¾æçº¿è™«ç—…é¢†åŸŸçš„ä¸“å®¶ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=300
                )
                return response.choices[0].message.content.strip()
            else:
                # Mockåˆ†æ
                return f"åŸºäºæ£€æµ‹ç»“æœï¼Œå‘ç°{len(entities)}ä¸ªç›¸å…³å®ä½“ã€‚å»ºè®®è¿›ä¸€æ­¥ç›‘æµ‹å¹¶é‡‡å–ç›¸åº”é˜²æ²»æªæ–½ã€‚"
                
        except Exception as e:
            logger.error(f"AIåˆ†æå¤±è´¥: {e}")
            return "AIåˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ£€æµ‹ç»“æœè¿›è¡Œäººå·¥åˆ†æã€‚"
    
    def _calculate_prediction_confidence(self, entities: List[Dict[str, Any]], disease_info: Dict[str, Any]) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        if not entities:
            return 0.0
        
        # åŸºäºå®ä½“è¯†åˆ«ç½®ä¿¡åº¦å’Œç–¾ç—…å…³è”åº¦è®¡ç®—
        entity_confidences = [e["confidence"] for e in entities]
        avg_confidence = np.mean(entity_confidences)
        
        # å¦‚æœæ‰¾åˆ°ç›¸å…³ç–¾ç—…ï¼Œå¢åŠ ç½®ä¿¡åº¦
        disease_bonus = 0.2 if disease_info.get("diseases") else 0.0
        
        return round(min(avg_confidence + disease_bonus, 1.0), 1)
    
    def _assess_risk_level(self, entities: List[Dict[str, Any]], disease_info: Dict[str, Any]) -> str:
        """è¯„ä¼°é£é™©ç­‰çº§"""
        high_risk_symptoms = ["æ¾é’ˆå˜çº¢", "æ¾é’ˆè„±è½", "æ ‘å¹²æµè„‚"]
        high_risk_insects = ["æ¾å¢¨å¤©ç‰›"]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜é£é™©æŒ‡æ ‡
        has_high_risk = any(
            entity["matched_kb_entity"] in high_risk_symptoms or 
            entity["matched_kb_entity"] in high_risk_insects
            for entity in entities
            if entity.get("matched_kb_entity")
        )
        
        if has_high_risk and disease_info.get("diseases"):
            return "é«˜é£é™©"
        elif has_high_risk or disease_info.get("diseases"):
            return "ä¸­é£é™©"
        else:
            return "ä½é£é™©"
    
    async def _identify_knowledge_gaps(self, entities: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """è¯†åˆ«çŸ¥è¯†ç¼ºå£"""
        gaps = []
        
        for entity in entities:
            # å¦‚æœç›¸ä¼¼åº¦è¾ƒä½ï¼Œå¯èƒ½æ˜¯æ–°çš„å®ä½“æˆ–å˜ç§
            if entity["similarity"] < self.confidence_threshold:
                gaps.append({
                    "type": "ä½åŒ¹é…å®ä½“",
                    "entity": entity["name"],
                    "similarity": entity["similarity"],
                    "suggestion": f"å»ºè®®å°†'{entity['name']}'æ·»åŠ åˆ°çŸ¥è¯†åº“ä¸­"
                })
        
        return gaps


# å…¨å±€æœåŠ¡å®ä¾‹
image_analysis_service = None
knowledge_inference_service = None


def init_image_services(db_config: Dict[str, Any]):
    """
    åˆå§‹åŒ–å›¾åƒæœåŠ¡
    
    Args:
        db_config: æ•°æ®åº“é…ç½®
    """
    global image_analysis_service, knowledge_inference_service
    
    image_analysis_service = ImageAnalysisService()
    knowledge_inference_service = KnowledgeInferenceService(db_config)
    
    logger.info("å›¾åƒæœåŠ¡åˆå§‹åŒ–å®Œæˆ")


def get_image_analysis_service() -> ImageAnalysisService:
    """è·å–å›¾åƒåˆ†ææœåŠ¡å®ä¾‹"""
    if image_analysis_service is None:
        raise RuntimeError("å›¾åƒåˆ†ææœåŠ¡æœªåˆå§‹åŒ–")
    return image_analysis_service


def get_knowledge_inference_service() -> KnowledgeInferenceService:
    """è·å–çŸ¥è¯†æ¨ç†æœåŠ¡å®ä¾‹"""
    if knowledge_inference_service is None:
        raise RuntimeError("çŸ¥è¯†æ¨ç†æœåŠ¡æœªåˆå§‹åŒ–")
    return knowledge_inference_service